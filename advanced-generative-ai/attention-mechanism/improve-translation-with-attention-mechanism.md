# Improve translation with Attention Mechanism

*

    <figure><img src="../../.gitbook/assets/image (11) (1).png" alt=""><figcaption></figcaption></figure>
* Alpha represents the attention weight at each time step
* H represents hidden state of encoder RNN at each time step
* HD respresent hidden state of decoder RNN at each time step
* During the attention step, we use the encoder hidden state to calculate to calculate context vector a
* We then concatenate a and H into a vector
* This vector is passed through a NN
* The output of the NN indicates the output word of this time step
* This process continues till the end of sentence token is generated by the decoder
